{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNIA 18/19 Project 2:  Gradient Descent & Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will implement a [multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression) model with tensorflow for Fashion-MNIST dataset. Cross Validation will be used to find the best **regularization parameter** $\\lambda$ for the L2-regularization term. Fashion-MNIST dataset is similar to the sklearn Digit dataset you used in the Project 1. It contains 60,000 training images and 10,000 testing images. Each example is a 28×28 grayscale image, associated with a label from 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural Network](https://s3-eu-central-1.amazonaws.com/zalando-wp-zalando-research-production/2017/08/fashion-mnist-sprite.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial logistic regression is a probabilistic, linear classifier. It is parametrized by a weight matrix $W$ and a bias vector $b$. Classification is done by projecting an input vector onto a set of hyperplanes, each of which corresponds to a class. The distance from the input to a hyperplane reflects the probability that the input is a member of the corresponding class.\n",
    "\n",
    "Mathematically, the probability that an input vector $\\bf{x} \\in \\mathbb{R}^p$ is a member of a class $i$ can be written as:\n",
    "$$P(Y=i|\\textbf{x}, W, b) = softmax(W\\textbf{x} + b)_i = \\frac{e^{W_i\\textbf{x} + b_i}}{\\sum_j{e^{W_j\\textbf{x} + b_j}}}$$\n",
    "where $W \\in \\mathbb{R}^{c \\times p}$, $b \\in \\mathbb{R}^c$ and $W_i \\in \\mathbb{R}^p$.\n",
    "\n",
    "The model’s prediction $y_{pred}$ is the class whose probability is maximal, specifically:\n",
    "$$y_{pred} = argmax_iP(Y=i|\\textbf{x}, W, b)$$\n",
    "\n",
    "We use cross-entropy loss with L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dataset and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load **Fashion-MNIST** dataset and normalized it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_trainval, Y_trainval), (X_test, Y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The X_trainval has the following shape:\n",
      "Rows: 60000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "X_trainval = np.reshape(X_trainval, (X_trainval.shape[0],  X_trainval.shape[1] *  X_trainval.shape[2]))\n",
    "print('The X_trainval has the following shape:')\n",
    "print('Rows: %d, columns: %d' % (X_trainval.shape[0], X_trainval.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The X_test has the following shape:\n",
      "Rows: 10000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "X_test = np.reshape(X_test, (X_test.shape[0],  X_test.shape[1] *  X_test.shape[2]))\n",
    "print('The X_test has the following shape:')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data. Subtract the mean and divide by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_normalization(X_trainval, X_test):\n",
    "    # TODO: Implement\n",
    "    \n",
    "    mean_X_trainval = np.mean(X_trainval, axis=0)  \n",
    "    std_X_trainval = np.std(a=X_trainval, axis=0)\n",
    "    mean_X_test  = np.mean(X_test, axis=0)  \n",
    "    std_X_test = np.std(a=X_test, axis=0)\n",
    "    X_trainval_normalized = np.divide(np.subtract(X_trainval, mean_X_trainval), std_X_trainval)\n",
    "    X_test_normalized = np.divide(np.subtract(X_test, mean_X_test), std_X_test)\n",
    "    return X_trainval_normalized, X_test_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7237471537470508e-18"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The normalization should be done on X_train and X_test. \n",
    "# The normalized data should have the exactly same shape as the original data matrix.\n",
    "\n",
    "X_trainval, X_test = data_normalization(X_trainval, X_test)\n",
    "np.mean(X_trainval[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define the Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here the global configuration of this program is \n",
    "# defined, which you shouldn't change.\n",
    "\n",
    "class global_config(object):\n",
    "    lr = 0.0001  # learning rate\n",
    "    img_h = 28  # image height\n",
    "    img_w = 28  # image width\n",
    "    num_class = 10  # number of classes\n",
    "    num_epoch = 20  # number of training epochs\n",
    "    batch_size = 16  # batch size\n",
    "    K = 3  # K-fold cross validation\n",
    "    num_train = None  # the number of training data\n",
    "    lambd = None  # the factor for the L2-regularization\n",
    "\n",
    "config = global_config()\n",
    "config.num_train = X_trainval.shape[0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_val_split(X_trainval, Y_trainval, i, K):\n",
    "    \"\"\"\n",
    "    sklearn library is not allowed to use here.\n",
    "    \n",
    "    K is the total number of folds and i is the current fold.\n",
    "    \n",
    "    Think about how to deal with the case when the number of \n",
    "    training data can't be divided by K evenly.\n",
    "    \"\"\"\n",
    "    #TODO: Implement\n",
    "    train_indices = np.array([], dtype=int)\n",
    "    fold_indices = CV_fold_indices(X=X_trainval, n_splits=K, shuffle=False, random_state=None)\n",
    "    \n",
    "    for idx, fold_idx in enumerate(fold_indices):\n",
    "        if(idx==i):\n",
    "            val_indices = fold_idx\n",
    "        else:\n",
    "            train_indices = np.append(train_indices, fold_idx)    \n",
    "    \n",
    "    X_val = X_trainval[val_indices]\n",
    "    Y_val = Y_trainval[val_indices]\n",
    "    X_train = X_trainval[train_indices]\n",
    "    Y_train = Y_trainval[train_indices]\n",
    "    \n",
    "    #print(\"X_train, X_val, Y_train, Y_val\", X_train.shape, X_val.shape, Y_train.shape, Y_val.shape)\n",
    "    \n",
    "    return X_train, X_val, Y_train, Y_val\n",
    "\n",
    "def CV_fold_indices(X, n_splits=3, shuffle=False, random_state=None):\n",
    "    n_samples = X.shape[0] \n",
    "    #print(\"n_samples:\", n_samples)\n",
    "    indices = np.arange(n_samples)\n",
    "    if shuffle:\n",
    "        check_random_state(random_state).shuffle(indices)\n",
    "\n",
    "    fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)\n",
    "    fold_sizes[:n_samples % n_splits] += 1  # This is one way of dealing when number of samples cannot be divided by K evenly.\n",
    "    current = 0\n",
    "    for fold_size in fold_sizes:\n",
    "        start, stop = current, current + fold_size\n",
    "        yield indices[start:stop]\n",
    "        current = stop\n",
    "        \n",
    "def encode_labels(y, k):\n",
    "    arr = np.zeros((len(y),k))\n",
    "    for i in range(len(y)):\n",
    "        arr[i][y[i]] = 1\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_train_data(X_train, Y_train):\n",
    "    \"\"\"called after each epoch\"\"\"\n",
    "    perm = np.random.permutation(len(Y_train))\n",
    "    Xtr_shuf = X_train[perm]\n",
    "    Ytr_shuf = Y_train[perm]\n",
    "    return Xtr_shuf, Ytr_shuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "training\n",
    "\"\"\"\n",
    "class logistic_regression(object):\n",
    "    \n",
    "    def __init__(self, X, Y_gt, config, name):\n",
    "        \"\"\"\n",
    "        :param X: the training batch, which has the shape [batch_size, n_features].\n",
    "        :param Y_gt: the corresponding ground truth label vector.\n",
    "        :param config: the hyper-parameters you need for the implementation.\n",
    "        :param name: the name of this logistic regression model which is used to\n",
    "                     avoid the naming confict with the help of tf.variable_scope and reuse.\n",
    "       \n",
    "        Define the computation graph within the variable_scope here. \n",
    "        First define two variables W and b with tf.get_variable.\n",
    "        Then do the forward pass.\n",
    "        Then compute the cross entropy loss with tensorflow, don't forget the L2-regularization.\n",
    "        The Adam optimizer is already given. You shouldn't change it.\n",
    "        Finally compute the accuracy for one batch\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
    "            #TODO: Define two variables and the forward pass.\n",
    "            W = tf.get_variable(\"W\", [config.num_class, (config.img_h*config.img_w)], initializer=tf.contrib.layers.xavier_initializer(seed=42))\n",
    "            b = tf.get_variable(\"b\", [config.num_class, 1], initializer=tf.zeros_initializer())\n",
    "            \n",
    "            # forward pass\n",
    "            z1 = tf.add(tf.matmul(X,tf.transpose(W)), tf.transpose(b))\n",
    "            \n",
    "            #TODO: Compute the cross entropy loss with L2-regularization.\n",
    "            self._loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=z1, labels=Y_gt)) + config.lambd*tf.nn.l2_loss(W)\n",
    "            \n",
    "            # Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent \n",
    "            # to update network weights iteratively.\n",
    "            # It will be introduced in the lecture when talking about the optimization algorithms.\n",
    "            self._train_step = tf.train.AdamOptimizer(config.lr).minimize(self._loss)\n",
    "            #TODO: Compute the accuracy\n",
    "            correct_prediction = tf.equal(tf.argmax(z1, axis=1), tf.argmax(Y_gt, axis=1))\n",
    "            self._num_acc = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "                        \n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_step\n",
    "    \n",
    "    @property\n",
    "    def loss(self):\n",
    "        return self._loss\n",
    "    \n",
    "    @property\n",
    "    def num_acc(self):\n",
    "        return self._num_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testing(model, X_test, Y_test, config):\n",
    "    \"\"\" \n",
    "    Go through the X_test and use sess.run() to compute the loss and accuracy.\n",
    "    \n",
    "    Return the total loss and the accuracy for X_test.\n",
    "    \n",
    "    Note that this function will be used for the validation data\n",
    "    during training and the test data after training.\n",
    "    \"\"\"\n",
    "    num_test = X_test.shape[0]\n",
    "    total_cost = 0\n",
    "    accs = 0\n",
    "    #TODO: Implement\n",
    "    total_cost = sess.run(model.loss, feed_dict={X: X_test, Y_gt: Y_test})\n",
    "    accs = sess.run(model.num_acc,feed_dict={X: X_test, Y_gt: Y_test})\n",
    "    # Note: cost is already averaged over number samples in tensorflow function tf.reduce_mean, thus dividing by len(Y_test)\n",
    "    # is not required.\n",
    "    return total_cost, accs\n",
    "    #return total_cost / len(Y_test), accs / len(Y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, X_train, X_val, Y_train, Y_val, config):\n",
    "    \"\"\"\n",
    "    Train the model with sess.run().\n",
    "    \n",
    "    You should shuffle the data after each epoch and\n",
    "    evaluate training and validation loss after each epoch.\n",
    "    \n",
    "    Return the lists of the training/validation loss and accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    cost_trains = []\n",
    "    acc_trains = []\n",
    "    cost_vals = []\n",
    "    acc_vals = []\n",
    "        \n",
    "    for i in range(config.num_epoch):\n",
    "       #TODO: Implement\n",
    "        cost_train = 0\n",
    "        cost_val = 0\n",
    "        acc_train = 0\n",
    "        acc_val = 0\n",
    "        num_minibatches = int(X_train.shape[0] // config.batch_size)\n",
    "        Xtr_shuf, Ytr_shuf = shuffle_train_data(X_train, Y_train)\n",
    "        Xval_shuf, Yval_shuf = shuffle_train_data(X_val, Y_val)\n",
    "        for n_batch in range(num_minibatches):\n",
    "            Xtr_batch = Xtr_shuf[n_batch*config.batch_size:min(((n_batch+1)*config.batch_size),len(X_train)),:]\n",
    "            Ytr_batch = Ytr_shuf[n_batch*config.batch_size:min(((n_batch+1)*config.batch_size),len(X_train))]\n",
    "            \n",
    "            Ytr_enc_label = encode_labels(Ytr_batch, k=config.num_class)\n",
    "            _, cost_train = sess.run([model.train_op, model.loss], feed_dict={X: Xtr_batch, Y_gt: Ytr_enc_label})\n",
    "        cost_trains.append(cost_train)\n",
    "        acc_train = sess.run(model.num_acc, feed_dict={X: Xtr_shuf, Y_gt: encode_labels(Ytr_shuf, k=config.num_class)})\n",
    "        acc_trains.append(acc_train)\n",
    "        print(\"Epoch: %d :\" % (i + 1))\n",
    "        print(\"Train Loss: %f\" %  cost_train)\n",
    "        print(\"Training acc: %f\" % acc_train)\n",
    "        \n",
    "        cost_val, acc_val = testing(model=model, X_test=Xval_shuf, Y_test=encode_labels(Yval_shuf, k=config.num_class), config=config)\n",
    "        cost_vals.append(cost_val)\n",
    "        acc_vals.append(acc_val)\n",
    "        print(\"Validation Loss: %f\" % cost_val)\n",
    "        print(\"Validation acc: %f\" % acc_val)\n",
    "    return cost_trains, acc_trains, cost_vals, acc_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement cross validation to find an optimal value of $\\lambda$. The optimal hyper-parameters should be determined by the validation accuracy. The test set should only be used in the very end after all other processing, e.g. hyper-parameter choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda is 100.000000\n",
      "Epoch: 1 :\n",
      "Train Loss: 2.272540\n",
      "Training acc: 0.658725\n",
      "Validation Loss: 2.191749\n",
      "Validation acc: 0.653850\n",
      "Epoch: 2 :\n",
      "Train Loss: 2.313624\n",
      "Training acc: 0.591175\n",
      "Validation Loss: 2.194850\n",
      "Validation acc: 0.587350\n",
      "Epoch: 3 :\n",
      "Train Loss: 2.221874\n",
      "Training acc: 0.591750\n",
      "Validation Loss: 2.203135\n",
      "Validation acc: 0.589400\n",
      "Epoch: 4 :\n",
      "Train Loss: 2.227205\n",
      "Training acc: 0.632400\n",
      "Validation Loss: 2.203675\n",
      "Validation acc: 0.629450\n",
      "Epoch: 5 :\n",
      "Train Loss: 2.190022\n",
      "Training acc: 0.578075\n",
      "Validation Loss: 2.207541\n",
      "Validation acc: 0.575500\n",
      "Epoch: 6 :\n",
      "Train Loss: 2.253126\n",
      "Training acc: 0.531350\n",
      "Validation Loss: 2.205642\n",
      "Validation acc: 0.527450\n",
      "Epoch: 7 :\n",
      "Train Loss: 2.191021\n",
      "Training acc: 0.623575\n",
      "Validation Loss: 2.199728\n",
      "Validation acc: 0.618400\n",
      "Epoch: 8 :\n",
      "Train Loss: 2.233153\n",
      "Training acc: 0.553950\n",
      "Validation Loss: 2.199090\n",
      "Validation acc: 0.551450\n",
      "Epoch: 9 :\n",
      "Train Loss: 2.184422\n",
      "Training acc: 0.599100\n",
      "Validation Loss: 2.202455\n",
      "Validation acc: 0.597700\n",
      "Epoch: 10 :\n",
      "Train Loss: 2.190851\n",
      "Training acc: 0.577075\n",
      "Validation Loss: 2.201557\n",
      "Validation acc: 0.575750\n",
      "Epoch: 11 :\n",
      "Train Loss: 2.143449\n",
      "Training acc: 0.559525\n",
      "Validation Loss: 2.198681\n",
      "Validation acc: 0.551300\n",
      "Epoch: 12 :\n",
      "Train Loss: 2.243553\n",
      "Training acc: 0.637050\n",
      "Validation Loss: 2.199008\n",
      "Validation acc: 0.634500\n",
      "Epoch: 13 :\n",
      "Train Loss: 2.218685\n",
      "Training acc: 0.563250\n",
      "Validation Loss: 2.202319\n",
      "Validation acc: 0.553850\n",
      "Epoch: 14 :\n",
      "Train Loss: 2.196586\n",
      "Training acc: 0.649300\n",
      "Validation Loss: 2.199672\n",
      "Validation acc: 0.647050\n",
      "Epoch: 15 :\n",
      "Train Loss: 2.175371\n",
      "Training acc: 0.589325\n",
      "Validation Loss: 2.205006\n",
      "Validation acc: 0.585800\n",
      "Epoch: 16 :\n",
      "Train Loss: 2.183549\n",
      "Training acc: 0.644050\n",
      "Validation Loss: 2.199862\n",
      "Validation acc: 0.639400\n",
      "Epoch: 17 :\n",
      "Train Loss: 2.220361\n",
      "Training acc: 0.619350\n",
      "Validation Loss: 2.197416\n",
      "Validation acc: 0.611750\n",
      "Epoch: 18 :\n",
      "Train Loss: 2.235830\n",
      "Training acc: 0.657475\n",
      "Validation Loss: 2.205799\n",
      "Validation acc: 0.649350\n",
      "Epoch: 19 :\n",
      "Train Loss: 2.202740\n",
      "Training acc: 0.623325\n",
      "Validation Loss: 2.199769\n",
      "Validation acc: 0.619150\n",
      "Epoch: 20 :\n",
      "Train Loss: 2.183207\n",
      "Training acc: 0.557100\n",
      "Validation Loss: 2.201580\n",
      "Validation acc: 0.555250\n",
      "Epoch: 1 :\n",
      "Train Loss: 2.200513\n",
      "Training acc: 0.618200\n",
      "Validation Loss: 2.192826\n",
      "Validation acc: 0.619850\n",
      "Epoch: 2 :\n",
      "Train Loss: 2.207337\n",
      "Training acc: 0.662900\n",
      "Validation Loss: 2.194313\n",
      "Validation acc: 0.663000\n",
      "Epoch: 3 :\n",
      "Train Loss: 2.186929\n",
      "Training acc: 0.569525\n",
      "Validation Loss: 2.199898\n",
      "Validation acc: 0.567050\n",
      "Epoch: 4 :\n",
      "Train Loss: 2.141865\n",
      "Training acc: 0.588500\n",
      "Validation Loss: 2.199430\n",
      "Validation acc: 0.589300\n",
      "Epoch: 5 :\n",
      "Train Loss: 2.205030\n",
      "Training acc: 0.595025\n",
      "Validation Loss: 2.201476\n",
      "Validation acc: 0.596000\n",
      "Epoch: 6 :\n",
      "Train Loss: 2.176550\n",
      "Training acc: 0.561550\n",
      "Validation Loss: 2.203722\n",
      "Validation acc: 0.559650\n",
      "Epoch: 7 :\n",
      "Train Loss: 2.191194\n",
      "Training acc: 0.630350\n",
      "Validation Loss: 2.201802\n",
      "Validation acc: 0.629250\n",
      "Epoch: 8 :\n",
      "Train Loss: 2.213799\n",
      "Training acc: 0.595700\n",
      "Validation Loss: 2.198378\n",
      "Validation acc: 0.595350\n",
      "Epoch: 9 :\n",
      "Train Loss: 2.120294\n",
      "Training acc: 0.590625\n",
      "Validation Loss: 2.200835\n",
      "Validation acc: 0.592100\n",
      "Epoch: 10 :\n",
      "Train Loss: 2.262454\n",
      "Training acc: 0.624250\n",
      "Validation Loss: 2.201007\n",
      "Validation acc: 0.623350\n",
      "Epoch: 11 :\n",
      "Train Loss: 2.197970\n",
      "Training acc: 0.638750\n",
      "Validation Loss: 2.197483\n",
      "Validation acc: 0.636150\n",
      "Epoch: 12 :\n",
      "Train Loss: 2.258519\n",
      "Training acc: 0.608625\n",
      "Validation Loss: 2.206664\n",
      "Validation acc: 0.606950\n",
      "Epoch: 13 :\n",
      "Train Loss: 2.232795\n",
      "Training acc: 0.579575\n",
      "Validation Loss: 2.202826\n",
      "Validation acc: 0.581300\n",
      "Epoch: 14 :\n",
      "Train Loss: 2.201812\n",
      "Training acc: 0.613200\n",
      "Validation Loss: 2.198358\n",
      "Validation acc: 0.613050\n",
      "Epoch: 15 :\n",
      "Train Loss: 2.180357\n",
      "Training acc: 0.642225\n",
      "Validation Loss: 2.199441\n",
      "Validation acc: 0.640250\n",
      "Epoch: 16 :\n",
      "Train Loss: 2.225665\n",
      "Training acc: 0.595900\n",
      "Validation Loss: 2.200580\n",
      "Validation acc: 0.595850\n",
      "Epoch: 17 :\n",
      "Train Loss: 2.126521\n",
      "Training acc: 0.608425\n",
      "Validation Loss: 2.199758\n",
      "Validation acc: 0.606050\n",
      "Epoch: 18 :\n",
      "Train Loss: 2.171022\n",
      "Training acc: 0.595550\n",
      "Validation Loss: 2.206303\n",
      "Validation acc: 0.596100\n",
      "Epoch: 19 :\n",
      "Train Loss: 2.247992\n",
      "Training acc: 0.557200\n",
      "Validation Loss: 2.203491\n",
      "Validation acc: 0.557400\n",
      "Epoch: 20 :\n",
      "Train Loss: 2.202863\n",
      "Training acc: 0.563200\n",
      "Validation Loss: 2.202609\n",
      "Validation acc: 0.563750\n",
      "Epoch: 1 :\n",
      "Train Loss: 2.197731\n",
      "Training acc: 0.635725\n",
      "Validation Loss: 2.191921\n",
      "Validation acc: 0.635550\n",
      "Epoch: 2 :\n",
      "Train Loss: 2.191293\n",
      "Training acc: 0.574625\n",
      "Validation Loss: 2.196584\n",
      "Validation acc: 0.580350\n",
      "Epoch: 3 :\n",
      "Train Loss: 2.273471\n",
      "Training acc: 0.589825\n",
      "Validation Loss: 2.199815\n",
      "Validation acc: 0.592950\n",
      "Epoch: 4 :\n",
      "Train Loss: 2.223117\n",
      "Training acc: 0.649050\n",
      "Validation Loss: 2.200704\n",
      "Validation acc: 0.649150\n",
      "Epoch: 5 :\n",
      "Train Loss: 2.176351\n",
      "Training acc: 0.639050\n",
      "Validation Loss: 2.198765\n",
      "Validation acc: 0.639650\n",
      "Epoch: 6 :\n",
      "Train Loss: 2.184657\n",
      "Training acc: 0.598100\n",
      "Validation Loss: 2.203033\n",
      "Validation acc: 0.599250\n",
      "Epoch: 7 :\n",
      "Train Loss: 2.171570\n",
      "Training acc: 0.637975\n",
      "Validation Loss: 2.199090\n",
      "Validation acc: 0.642450\n",
      "Epoch: 8 :\n",
      "Train Loss: 2.174060\n",
      "Training acc: 0.633350\n",
      "Validation Loss: 2.197945\n",
      "Validation acc: 0.636800\n",
      "Epoch: 9 :\n",
      "Train Loss: 2.172422\n",
      "Training acc: 0.631275\n",
      "Validation Loss: 2.199579\n",
      "Validation acc: 0.632150\n",
      "Epoch: 10 :\n",
      "Train Loss: 2.185362\n",
      "Training acc: 0.648325\n",
      "Validation Loss: 2.198334\n",
      "Validation acc: 0.648150\n",
      "Epoch: 11 :\n",
      "Train Loss: 2.201504\n",
      "Training acc: 0.586250\n",
      "Validation Loss: 2.203166\n",
      "Validation acc: 0.590550\n",
      "Epoch: 12 :\n",
      "Train Loss: 2.149561\n",
      "Training acc: 0.609075\n",
      "Validation Loss: 2.208345\n",
      "Validation acc: 0.607700\n",
      "Epoch: 13 :\n",
      "Train Loss: 2.248999\n",
      "Training acc: 0.575975\n",
      "Validation Loss: 2.200948\n",
      "Validation acc: 0.575900\n",
      "Epoch: 14 :\n",
      "Train Loss: 2.221997\n",
      "Training acc: 0.591150\n",
      "Validation Loss: 2.200910\n",
      "Validation acc: 0.593550\n",
      "Epoch: 15 :\n",
      "Train Loss: 2.139494\n",
      "Training acc: 0.614150\n",
      "Validation Loss: 2.204347\n",
      "Validation acc: 0.618400\n",
      "Epoch: 16 :\n",
      "Train Loss: 2.213715\n",
      "Training acc: 0.510500\n",
      "Validation Loss: 2.206870\n",
      "Validation acc: 0.513000\n",
      "Epoch: 17 :\n",
      "Train Loss: 2.218652\n",
      "Training acc: 0.614800\n",
      "Validation Loss: 2.204129\n",
      "Validation acc: 0.612650\n",
      "Epoch: 18 :\n",
      "Train Loss: 2.181281\n",
      "Training acc: 0.622900\n",
      "Validation Loss: 2.198301\n",
      "Validation acc: 0.616850\n",
      "Epoch: 19 :\n",
      "Train Loss: 2.236952\n",
      "Training acc: 0.592175\n",
      "Validation Loss: 2.199334\n",
      "Validation acc: 0.591300\n",
      "Epoch: 20 :\n",
      "Train Loss: 2.233294\n",
      "Training acc: 0.558600\n",
      "Validation Loss: 2.202054\n",
      "Validation acc: 0.555850\n",
      "The validation loss for lambda 100.000000 is 2.202081\n",
      "lambda is 1.000000\n",
      "Epoch: 1 :\n",
      "Train Loss: 1.076378\n",
      "Training acc: 0.773275\n",
      "Validation Loss: 1.031100\n",
      "Validation acc: 0.768900\n",
      "Epoch: 2 :\n",
      "Train Loss: 0.989391\n",
      "Training acc: 0.773300\n",
      "Validation Loss: 1.017737\n",
      "Validation acc: 0.770300\n",
      "Epoch: 3 :\n",
      "Train Loss: 0.803565\n",
      "Training acc: 0.776000\n",
      "Validation Loss: 1.015709\n",
      "Validation acc: 0.773300\n",
      "Epoch: 4 :\n",
      "Train Loss: 0.994994\n",
      "Training acc: 0.773325\n",
      "Validation Loss: 1.015080\n",
      "Validation acc: 0.768850\n",
      "Epoch: 5 :\n",
      "Train Loss: 0.771174\n",
      "Training acc: 0.774825\n",
      "Validation Loss: 1.010736\n",
      "Validation acc: 0.771900\n",
      "Epoch: 6 :\n",
      "Train Loss: 0.788503\n",
      "Training acc: 0.778725\n",
      "Validation Loss: 1.010406\n",
      "Validation acc: 0.776450\n",
      "Epoch: 7 :\n",
      "Train Loss: 0.984623\n",
      "Training acc: 0.777825\n",
      "Validation Loss: 1.007754\n",
      "Validation acc: 0.774750\n",
      "Epoch: 8 :\n",
      "Train Loss: 1.201566\n",
      "Training acc: 0.775800\n",
      "Validation Loss: 1.010298\n",
      "Validation acc: 0.772050\n",
      "Epoch: 9 :\n",
      "Train Loss: 0.902082\n",
      "Training acc: 0.776000\n",
      "Validation Loss: 1.008446\n",
      "Validation acc: 0.774350\n",
      "Epoch: 10 :\n",
      "Train Loss: 0.897882\n",
      "Training acc: 0.781325\n",
      "Validation Loss: 1.005486\n",
      "Validation acc: 0.778900\n",
      "Epoch: 11 :\n",
      "Train Loss: 0.973192\n",
      "Training acc: 0.781400\n",
      "Validation Loss: 1.005632\n",
      "Validation acc: 0.778650\n",
      "Epoch: 12 :\n",
      "Train Loss: 0.925720\n",
      "Training acc: 0.775100\n",
      "Validation Loss: 1.010168\n",
      "Validation acc: 0.772500\n",
      "Epoch: 13 :\n",
      "Train Loss: 0.934621\n",
      "Training acc: 0.776100\n",
      "Validation Loss: 1.004897\n",
      "Validation acc: 0.772950\n",
      "Epoch: 14 :\n",
      "Train Loss: 1.036673\n",
      "Training acc: 0.776950\n",
      "Validation Loss: 1.005427\n",
      "Validation acc: 0.775600\n",
      "Epoch: 15 :\n",
      "Train Loss: 1.093171\n",
      "Training acc: 0.773925\n",
      "Validation Loss: 1.006393\n",
      "Validation acc: 0.772450\n",
      "Epoch: 16 :\n",
      "Train Loss: 1.003453\n",
      "Training acc: 0.783350\n",
      "Validation Loss: 1.004406\n",
      "Validation acc: 0.780350\n",
      "Epoch: 17 :\n",
      "Train Loss: 1.392165\n",
      "Training acc: 0.773425\n",
      "Validation Loss: 1.008318\n",
      "Validation acc: 0.770400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 :\n",
      "Train Loss: 1.214007\n",
      "Training acc: 0.778700\n",
      "Validation Loss: 1.005943\n",
      "Validation acc: 0.774500\n",
      "Epoch: 19 :\n",
      "Train Loss: 1.005856\n",
      "Training acc: 0.777600\n",
      "Validation Loss: 1.006707\n",
      "Validation acc: 0.775550\n",
      "Epoch: 20 :\n",
      "Train Loss: 1.125405\n",
      "Training acc: 0.780375\n",
      "Validation Loss: 1.005095\n",
      "Validation acc: 0.775950\n",
      "Epoch: 1 :\n",
      "Train Loss: 0.954855\n",
      "Training acc: 0.767400\n",
      "Validation Loss: 1.027739\n",
      "Validation acc: 0.766450\n",
      "Epoch: 2 :\n",
      "Train Loss: 0.850413\n",
      "Training acc: 0.775825\n",
      "Validation Loss: 1.013795\n",
      "Validation acc: 0.778000\n",
      "Epoch: 3 :\n",
      "Train Loss: 1.097554\n",
      "Training acc: 0.772450\n",
      "Validation Loss: 1.014519\n",
      "Validation acc: 0.775650\n",
      "Epoch: 4 :\n",
      "Train Loss: 0.989608\n",
      "Training acc: 0.772950\n",
      "Validation Loss: 1.011914\n",
      "Validation acc: 0.775100\n",
      "Epoch: 5 :\n",
      "Train Loss: 1.191717\n",
      "Training acc: 0.773500\n",
      "Validation Loss: 1.010062\n",
      "Validation acc: 0.775700\n",
      "Epoch: 6 :\n",
      "Train Loss: 0.912674\n",
      "Training acc: 0.777100\n",
      "Validation Loss: 1.006704\n",
      "Validation acc: 0.778450\n",
      "Epoch: 7 :\n",
      "Train Loss: 0.832242\n",
      "Training acc: 0.776550\n",
      "Validation Loss: 1.005427\n",
      "Validation acc: 0.779250\n",
      "Epoch: 8 :\n",
      "Train Loss: 0.783963\n",
      "Training acc: 0.772075\n",
      "Validation Loss: 1.007803\n",
      "Validation acc: 0.773800\n",
      "Epoch: 9 :\n",
      "Train Loss: 0.974556\n",
      "Training acc: 0.777675\n",
      "Validation Loss: 1.004861\n",
      "Validation acc: 0.778950\n",
      "Epoch: 10 :\n",
      "Train Loss: 1.222028\n",
      "Training acc: 0.776225\n",
      "Validation Loss: 1.004278\n",
      "Validation acc: 0.777500\n",
      "Epoch: 11 :\n",
      "Train Loss: 1.030430\n",
      "Training acc: 0.779125\n",
      "Validation Loss: 1.003688\n",
      "Validation acc: 0.778750\n",
      "Epoch: 12 :\n",
      "Train Loss: 1.043755\n",
      "Training acc: 0.777050\n",
      "Validation Loss: 1.000913\n",
      "Validation acc: 0.778250\n",
      "Epoch: 13 :\n",
      "Train Loss: 0.973115\n",
      "Training acc: 0.778450\n",
      "Validation Loss: 1.001975\n",
      "Validation acc: 0.780400\n",
      "Epoch: 14 :\n",
      "Train Loss: 1.193787\n",
      "Training acc: 0.775475\n",
      "Validation Loss: 1.002582\n",
      "Validation acc: 0.777700\n",
      "Epoch: 15 :\n",
      "Train Loss: 0.833092\n",
      "Training acc: 0.778975\n",
      "Validation Loss: 1.003034\n",
      "Validation acc: 0.779700\n",
      "Epoch: 16 :\n",
      "Train Loss: 1.221707\n",
      "Training acc: 0.777650\n",
      "Validation Loss: 1.001270\n",
      "Validation acc: 0.777800\n",
      "Epoch: 17 :\n",
      "Train Loss: 1.004758\n",
      "Training acc: 0.779275\n",
      "Validation Loss: 1.002568\n",
      "Validation acc: 0.780950\n",
      "Epoch: 18 :\n",
      "Train Loss: 0.949389\n",
      "Training acc: 0.774475\n",
      "Validation Loss: 1.001073\n",
      "Validation acc: 0.775450\n",
      "Epoch: 19 :\n",
      "Train Loss: 1.208426\n",
      "Training acc: 0.775600\n",
      "Validation Loss: 1.000468\n",
      "Validation acc: 0.777850\n",
      "Epoch: 20 :\n",
      "Train Loss: 0.934077\n",
      "Training acc: 0.779375\n",
      "Validation Loss: 1.003433\n",
      "Validation acc: 0.781300\n",
      "Epoch: 1 :\n",
      "Train Loss: 0.842049\n",
      "Training acc: 0.770975\n",
      "Validation Loss: 1.021238\n",
      "Validation acc: 0.769750\n",
      "Epoch: 2 :\n",
      "Train Loss: 1.170248\n",
      "Training acc: 0.775575\n",
      "Validation Loss: 1.013661\n",
      "Validation acc: 0.771850\n",
      "Epoch: 3 :\n",
      "Train Loss: 1.294788\n",
      "Training acc: 0.772675\n",
      "Validation Loss: 1.010154\n",
      "Validation acc: 0.771050\n",
      "Epoch: 4 :\n",
      "Train Loss: 0.822611\n",
      "Training acc: 0.776125\n",
      "Validation Loss: 1.008918\n",
      "Validation acc: 0.771250\n",
      "Epoch: 5 :\n",
      "Train Loss: 0.839066\n",
      "Training acc: 0.777500\n",
      "Validation Loss: 1.004777\n",
      "Validation acc: 0.776850\n",
      "Epoch: 6 :\n",
      "Train Loss: 0.775669\n",
      "Training acc: 0.777475\n",
      "Validation Loss: 1.005336\n",
      "Validation acc: 0.775200\n",
      "Epoch: 7 :\n",
      "Train Loss: 0.779313\n",
      "Training acc: 0.777450\n",
      "Validation Loss: 1.003567\n",
      "Validation acc: 0.775250\n",
      "Epoch: 8 :\n",
      "Train Loss: 0.799300\n",
      "Training acc: 0.777875\n",
      "Validation Loss: 1.002184\n",
      "Validation acc: 0.775950\n",
      "Epoch: 9 :\n",
      "Train Loss: 0.760762\n",
      "Training acc: 0.776350\n",
      "Validation Loss: 1.005270\n",
      "Validation acc: 0.775950\n",
      "Epoch: 10 :\n",
      "Train Loss: 1.135266\n",
      "Training acc: 0.781275\n",
      "Validation Loss: 1.002811\n",
      "Validation acc: 0.778200\n",
      "Epoch: 11 :\n",
      "Train Loss: 0.864099\n",
      "Training acc: 0.779125\n",
      "Validation Loss: 1.001474\n",
      "Validation acc: 0.780050\n",
      "Epoch: 12 :\n",
      "Train Loss: 0.770639\n",
      "Training acc: 0.772700\n",
      "Validation Loss: 1.004198\n",
      "Validation acc: 0.770850\n",
      "Epoch: 13 :\n",
      "Train Loss: 0.697443\n",
      "Training acc: 0.775525\n",
      "Validation Loss: 1.000930\n",
      "Validation acc: 0.773750\n",
      "Epoch: 14 :\n",
      "Train Loss: 1.338842\n",
      "Training acc: 0.775100\n",
      "Validation Loss: 1.002297\n",
      "Validation acc: 0.777550\n",
      "Epoch: 15 :\n",
      "Train Loss: 1.191444\n",
      "Training acc: 0.780450\n",
      "Validation Loss: 1.002488\n",
      "Validation acc: 0.778250\n",
      "Epoch: 16 :\n",
      "Train Loss: 0.969779\n",
      "Training acc: 0.780125\n",
      "Validation Loss: 1.003112\n",
      "Validation acc: 0.775950\n",
      "Epoch: 17 :\n",
      "Train Loss: 0.799466\n",
      "Training acc: 0.773575\n",
      "Validation Loss: 1.000677\n",
      "Validation acc: 0.771650\n",
      "Epoch: 18 :\n",
      "Train Loss: 1.059738\n",
      "Training acc: 0.781050\n",
      "Validation Loss: 1.000392\n",
      "Validation acc: 0.776450\n",
      "Epoch: 19 :\n",
      "Train Loss: 1.202102\n",
      "Training acc: 0.779275\n",
      "Validation Loss: 0.999454\n",
      "Validation acc: 0.778350\n",
      "Epoch: 20 :\n",
      "Train Loss: 1.056153\n",
      "Training acc: 0.779250\n",
      "Validation Loss: 0.999517\n",
      "Validation acc: 0.776600\n",
      "The validation loss for lambda 1.000000 is 1.002681\n",
      "lambda is 0.100000\n",
      "Epoch: 1 :\n",
      "Train Loss: 1.146564\n",
      "Training acc: 0.812075\n",
      "Validation Loss: 0.971819\n",
      "Validation acc: 0.806750\n",
      "Epoch: 2 :\n",
      "Train Loss: 1.021930\n",
      "Training acc: 0.832575\n",
      "Validation Loss: 0.726546\n",
      "Validation acc: 0.826050\n",
      "Epoch: 3 :\n",
      "Train Loss: 0.758570\n",
      "Training acc: 0.835800\n",
      "Validation Loss: 0.662179\n",
      "Validation acc: 0.826200\n",
      "Epoch: 4 :\n",
      "Train Loss: 0.731338\n",
      "Training acc: 0.836350\n",
      "Validation Loss: 0.645393\n",
      "Validation acc: 0.828750\n",
      "Epoch: 5 :\n",
      "Train Loss: 0.690040\n",
      "Training acc: 0.839250\n",
      "Validation Loss: 0.637820\n",
      "Validation acc: 0.831300\n",
      "Epoch: 6 :\n",
      "Train Loss: 0.782362\n",
      "Training acc: 0.835875\n",
      "Validation Loss: 0.637497\n",
      "Validation acc: 0.827550\n",
      "Epoch: 7 :\n",
      "Train Loss: 0.354103\n",
      "Training acc: 0.836150\n",
      "Validation Loss: 0.632246\n",
      "Validation acc: 0.828200\n",
      "Epoch: 8 :\n",
      "Train Loss: 0.343566\n",
      "Training acc: 0.836425\n",
      "Validation Loss: 0.634578\n",
      "Validation acc: 0.828100\n",
      "Epoch: 9 :\n",
      "Train Loss: 0.938184\n",
      "Training acc: 0.838725\n",
      "Validation Loss: 0.632874\n",
      "Validation acc: 0.830900\n",
      "Epoch: 10 :\n",
      "Train Loss: 1.053075\n",
      "Training acc: 0.836375\n",
      "Validation Loss: 0.633052\n",
      "Validation acc: 0.829600\n",
      "Epoch: 11 :\n",
      "Train Loss: 0.420379\n",
      "Training acc: 0.835300\n",
      "Validation Loss: 0.635495\n",
      "Validation acc: 0.828950\n",
      "Epoch: 12 :\n",
      "Train Loss: 0.580645\n",
      "Training acc: 0.837100\n",
      "Validation Loss: 0.630321\n",
      "Validation acc: 0.829250\n",
      "Epoch: 13 :\n",
      "Train Loss: 0.803318\n",
      "Training acc: 0.838825\n",
      "Validation Loss: 0.629902\n",
      "Validation acc: 0.831550\n",
      "Epoch: 14 :\n",
      "Train Loss: 0.408181\n",
      "Training acc: 0.836950\n",
      "Validation Loss: 0.634033\n",
      "Validation acc: 0.829600\n",
      "Epoch: 15 :\n",
      "Train Loss: 0.751865\n",
      "Training acc: 0.839950\n",
      "Validation Loss: 0.629695\n",
      "Validation acc: 0.832150\n",
      "Epoch: 16 :\n",
      "Train Loss: 0.622304\n",
      "Training acc: 0.833600\n",
      "Validation Loss: 0.630675\n",
      "Validation acc: 0.828500\n",
      "Epoch: 17 :\n",
      "Train Loss: 0.527609\n",
      "Training acc: 0.836325\n",
      "Validation Loss: 0.630720\n",
      "Validation acc: 0.829300\n",
      "Epoch: 18 :\n",
      "Train Loss: 0.699346\n",
      "Training acc: 0.836475\n",
      "Validation Loss: 0.629776\n",
      "Validation acc: 0.830500\n",
      "Epoch: 19 :\n",
      "Train Loss: 0.569676\n",
      "Training acc: 0.838375\n",
      "Validation Loss: 0.629315\n",
      "Validation acc: 0.830200\n",
      "Epoch: 20 :\n",
      "Train Loss: 0.952675\n",
      "Training acc: 0.835075\n",
      "Validation Loss: 0.629968\n",
      "Validation acc: 0.827550\n",
      "Epoch: 1 :\n",
      "Train Loss: 0.755722\n",
      "Training acc: 0.809875\n",
      "Validation Loss: 0.966685\n",
      "Validation acc: 0.812150\n",
      "Epoch: 2 :\n",
      "Train Loss: 0.616170\n",
      "Training acc: 0.824725\n",
      "Validation Loss: 0.723552\n",
      "Validation acc: 0.828050\n",
      "Epoch: 3 :\n",
      "Train Loss: 0.612511\n",
      "Training acc: 0.831775\n",
      "Validation Loss: 0.658814\n",
      "Validation acc: 0.833750\n",
      "Epoch: 4 :\n",
      "Train Loss: 0.339295\n",
      "Training acc: 0.834225\n",
      "Validation Loss: 0.636607\n",
      "Validation acc: 0.836000\n",
      "Epoch: 5 :\n",
      "Train Loss: 0.562029\n",
      "Training acc: 0.834125\n",
      "Validation Loss: 0.631684\n",
      "Validation acc: 0.835500\n",
      "Epoch: 6 :\n",
      "Train Loss: 0.561424\n",
      "Training acc: 0.835650\n",
      "Validation Loss: 0.629885\n",
      "Validation acc: 0.837750\n",
      "Epoch: 7 :\n",
      "Train Loss: 0.356383\n",
      "Training acc: 0.836075\n",
      "Validation Loss: 0.627668\n",
      "Validation acc: 0.837850\n",
      "Epoch: 8 :\n",
      "Train Loss: 0.632971\n",
      "Training acc: 0.833325\n",
      "Validation Loss: 0.629866\n",
      "Validation acc: 0.833700\n",
      "Epoch: 9 :\n",
      "Train Loss: 0.975640\n",
      "Training acc: 0.832975\n",
      "Validation Loss: 0.628392\n",
      "Validation acc: 0.832900\n",
      "Epoch: 10 :\n",
      "Train Loss: 0.592938\n",
      "Training acc: 0.829875\n",
      "Validation Loss: 0.629544\n",
      "Validation acc: 0.831700\n",
      "Epoch: 11 :\n",
      "Train Loss: 0.658913\n",
      "Training acc: 0.836700\n",
      "Validation Loss: 0.625611\n",
      "Validation acc: 0.836450\n",
      "Epoch: 12 :\n",
      "Train Loss: 0.586270\n",
      "Training acc: 0.835325\n",
      "Validation Loss: 0.628312\n",
      "Validation acc: 0.836300\n",
      "Epoch: 13 :\n",
      "Train Loss: 0.295409\n",
      "Training acc: 0.831225\n",
      "Validation Loss: 0.628340\n",
      "Validation acc: 0.832250\n",
      "Epoch: 14 :\n",
      "Train Loss: 0.511941\n",
      "Training acc: 0.833500\n",
      "Validation Loss: 0.626650\n",
      "Validation acc: 0.834050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 :\n",
      "Train Loss: 0.646855\n",
      "Training acc: 0.834750\n",
      "Validation Loss: 0.625040\n",
      "Validation acc: 0.835300\n",
      "Epoch: 16 :\n",
      "Train Loss: 0.432612\n",
      "Training acc: 0.832625\n",
      "Validation Loss: 0.626623\n",
      "Validation acc: 0.833150\n",
      "Epoch: 17 :\n",
      "Train Loss: 0.779698\n",
      "Training acc: 0.833475\n",
      "Validation Loss: 0.625354\n",
      "Validation acc: 0.835400\n",
      "Epoch: 18 :\n",
      "Train Loss: 0.673060\n",
      "Training acc: 0.836000\n",
      "Validation Loss: 0.625592\n",
      "Validation acc: 0.836350\n",
      "Epoch: 19 :\n",
      "Train Loss: 0.554787\n",
      "Training acc: 0.832075\n",
      "Validation Loss: 0.629431\n",
      "Validation acc: 0.833200\n",
      "Epoch: 20 :\n",
      "Train Loss: 0.420705\n",
      "Training acc: 0.835775\n",
      "Validation Loss: 0.625558\n",
      "Validation acc: 0.836000\n",
      "Epoch: 1 :\n",
      "Train Loss: 1.281042\n",
      "Training acc: 0.812000\n",
      "Validation Loss: 0.967221\n",
      "Validation acc: 0.807200\n",
      "Epoch: 2 :\n",
      "Train Loss: 0.793460\n",
      "Training acc: 0.828175\n",
      "Validation Loss: 0.720943\n",
      "Validation acc: 0.822900\n",
      "Epoch: 3 :\n",
      "Train Loss: 0.459140\n",
      "Training acc: 0.831300\n",
      "Validation Loss: 0.658134\n",
      "Validation acc: 0.826300\n",
      "Epoch: 4 :\n",
      "Train Loss: 0.894473\n",
      "Training acc: 0.838175\n",
      "Validation Loss: 0.638163\n",
      "Validation acc: 0.833500\n",
      "Epoch: 5 :\n",
      "Train Loss: 0.509401\n",
      "Training acc: 0.835325\n",
      "Validation Loss: 0.632616\n",
      "Validation acc: 0.831300\n",
      "Epoch: 6 :\n",
      "Train Loss: 0.731570\n",
      "Training acc: 0.836325\n",
      "Validation Loss: 0.630761\n",
      "Validation acc: 0.831400\n",
      "Epoch: 7 :\n",
      "Train Loss: 0.847054\n",
      "Training acc: 0.833875\n",
      "Validation Loss: 0.629117\n",
      "Validation acc: 0.828650\n",
      "Epoch: 8 :\n",
      "Train Loss: 0.798503\n",
      "Training acc: 0.838000\n",
      "Validation Loss: 0.626593\n",
      "Validation acc: 0.833600\n",
      "Epoch: 9 :\n",
      "Train Loss: 0.762139\n",
      "Training acc: 0.835975\n",
      "Validation Loss: 0.631598\n",
      "Validation acc: 0.831000\n",
      "Epoch: 10 :\n",
      "Train Loss: 0.698296\n",
      "Training acc: 0.834775\n",
      "Validation Loss: 0.627994\n",
      "Validation acc: 0.830800\n",
      "Epoch: 11 :\n",
      "Train Loss: 0.728077\n",
      "Training acc: 0.833850\n",
      "Validation Loss: 0.628978\n",
      "Validation acc: 0.826200\n",
      "Epoch: 12 :\n",
      "Train Loss: 0.485905\n",
      "Training acc: 0.836225\n",
      "Validation Loss: 0.625223\n",
      "Validation acc: 0.830800\n",
      "Epoch: 13 :\n",
      "Train Loss: 0.938910\n",
      "Training acc: 0.836525\n",
      "Validation Loss: 0.628620\n",
      "Validation acc: 0.831700\n",
      "Epoch: 14 :\n",
      "Train Loss: 0.498847\n",
      "Training acc: 0.835400\n",
      "Validation Loss: 0.630662\n",
      "Validation acc: 0.831200\n",
      "Epoch: 15 :\n",
      "Train Loss: 0.803131\n",
      "Training acc: 0.836050\n",
      "Validation Loss: 0.623989\n",
      "Validation acc: 0.832600\n",
      "Epoch: 16 :\n",
      "Train Loss: 0.695793\n",
      "Training acc: 0.832750\n",
      "Validation Loss: 0.627948\n",
      "Validation acc: 0.827850\n",
      "Epoch: 17 :\n",
      "Train Loss: 0.418195\n",
      "Training acc: 0.839550\n",
      "Validation Loss: 0.624968\n",
      "Validation acc: 0.834100\n",
      "Epoch: 18 :\n",
      "Train Loss: 0.538004\n",
      "Training acc: 0.834925\n",
      "Validation Loss: 0.624207\n",
      "Validation acc: 0.830150\n",
      "Epoch: 19 :\n",
      "Train Loss: 0.605306\n",
      "Training acc: 0.837700\n",
      "Validation Loss: 0.624147\n",
      "Validation acc: 0.834450\n",
      "Epoch: 20 :\n",
      "Train Loss: 1.116046\n",
      "Training acc: 0.835075\n",
      "Validation Loss: 0.626677\n",
      "Validation acc: 0.828700\n",
      "The validation loss for lambda 0.100000 is 0.627401\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Initialization\n",
    "\"\"\"\n",
    "# Use cross validation to choose the best lambda for the L2-regularization from the list below\n",
    "lambda_list = [100, 1, 0.1]\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, config.img_h * config.img_w])\n",
    "Y_gt = tf.placeholder(tf.int64, [None, config.num_class])\n",
    "\n",
    "for lambd in lambda_list:\n",
    "    val_loss_list = []\n",
    "    config.lambd = lambd\n",
    "    print(\"lambda is %f\" % lambd)\n",
    "    \n",
    "    for i in range(config.K):\n",
    "        # Prepare the training and validation data\n",
    "        X_train, X_val, Y_train, Y_val = train_val_split(X_trainval, Y_trainval, i, config.K)\n",
    "        \n",
    "        # For each lambda and K, we build a new model and train it from scratch\n",
    "        model = logistic_regression(X, Y_gt, config\n",
    "                                    ,name=str(lambd)+'_'+str(config.K))\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            # Initialize the variables of the model\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            \n",
    "            # Train the model\n",
    "            cost_trains, acc_trains, cost_vals, acc_vals = train(model, X_train, X_val, Y_train, Y_val, config)\n",
    "            \n",
    "        val_loss_list.append(cost_vals[-1])\n",
    "        \n",
    "    print(\"The validation loss for lambda %f is %f\" % (lambd, np.mean(val_loss_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Combine Train and Validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the hyper-parameters you choose from the cross validation to re-train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 :\n",
      "Train Loss: 0.791572\n",
      "Training acc: 0.825567\n",
      "Validation Loss: 0.824275\n",
      "Validation acc: 0.815600\n",
      "Epoch: 2 :\n",
      "Train Loss: 0.543899\n",
      "Training acc: 0.832467\n",
      "Validation Loss: 0.681321\n",
      "Validation acc: 0.820200\n",
      "Epoch: 3 :\n",
      "Train Loss: 0.415380\n",
      "Training acc: 0.834017\n",
      "Validation Loss: 0.658659\n",
      "Validation acc: 0.819600\n",
      "Epoch: 4 :\n",
      "Train Loss: 0.575336\n",
      "Training acc: 0.836383\n",
      "Validation Loss: 0.648015\n",
      "Validation acc: 0.823300\n",
      "Epoch: 5 :\n",
      "Train Loss: 0.617436\n",
      "Training acc: 0.832967\n",
      "Validation Loss: 0.651748\n",
      "Validation acc: 0.818900\n",
      "Epoch: 6 :\n",
      "Train Loss: 0.853357\n",
      "Training acc: 0.833817\n",
      "Validation Loss: 0.648201\n",
      "Validation acc: 0.822400\n",
      "Epoch: 7 :\n",
      "Train Loss: 0.537748\n",
      "Training acc: 0.835217\n",
      "Validation Loss: 0.647944\n",
      "Validation acc: 0.821200\n",
      "Epoch: 8 :\n",
      "Train Loss: 0.560548\n",
      "Training acc: 0.833950\n",
      "Validation Loss: 0.647417\n",
      "Validation acc: 0.821900\n",
      "Epoch: 9 :\n",
      "Train Loss: 0.513177\n",
      "Training acc: 0.832667\n",
      "Validation Loss: 0.647321\n",
      "Validation acc: 0.821200\n",
      "Epoch: 10 :\n",
      "Train Loss: 1.141453\n",
      "Training acc: 0.838283\n",
      "Validation Loss: 0.648064\n",
      "Validation acc: 0.825700\n",
      "Epoch: 11 :\n",
      "Train Loss: 0.956648\n",
      "Training acc: 0.835400\n",
      "Validation Loss: 0.646531\n",
      "Validation acc: 0.822300\n",
      "Epoch: 12 :\n",
      "Train Loss: 0.555194\n",
      "Training acc: 0.833167\n",
      "Validation Loss: 0.644503\n",
      "Validation acc: 0.822200\n",
      "Epoch: 13 :\n",
      "Train Loss: 0.476853\n",
      "Training acc: 0.837017\n",
      "Validation Loss: 0.644208\n",
      "Validation acc: 0.824000\n",
      "Epoch: 14 :\n",
      "Train Loss: 0.434415\n",
      "Training acc: 0.834700\n",
      "Validation Loss: 0.643430\n",
      "Validation acc: 0.820500\n",
      "Epoch: 15 :\n",
      "Train Loss: 0.360154\n",
      "Training acc: 0.836533\n",
      "Validation Loss: 0.649401\n",
      "Validation acc: 0.822800\n",
      "Epoch: 16 :\n",
      "Train Loss: 0.675797\n",
      "Training acc: 0.836133\n",
      "Validation Loss: 0.643618\n",
      "Validation acc: 0.822000\n",
      "Epoch: 17 :\n",
      "Train Loss: 0.583229\n",
      "Training acc: 0.835383\n",
      "Validation Loss: 0.644843\n",
      "Validation acc: 0.821900\n",
      "Epoch: 18 :\n",
      "Train Loss: 0.692263\n",
      "Training acc: 0.833183\n",
      "Validation Loss: 0.644060\n",
      "Validation acc: 0.821000\n",
      "Epoch: 19 :\n",
      "Train Loss: 0.640834\n",
      "Training acc: 0.830667\n",
      "Validation Loss: 0.647189\n",
      "Validation acc: 0.817400\n",
      "Epoch: 20 :\n",
      "Train Loss: 0.706491\n",
      "Training acc: 0.834167\n",
      "Validation Loss: 0.643477\n",
      "Validation acc: 0.823100\n",
      "The final test acc is 0.823100\n"
     ]
    }
   ],
   "source": [
    "config.lambd =  0.1 #TODO: Choose the best lambda\n",
    "\n",
    "model = logistic_regression(X, Y_gt, config, name='trainval')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    cost_trains, acc_trains, cost_tests, acc_tests = train(model, X_trainval, X_test, Y_trainval, Y_test, config)\n",
    "\n",
    "print(\"The final test acc is %f\" % acc_tests[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "costs_list_all = []\n",
    "\n",
    "class Fully_connected_Neural_Network(object):\n",
    "    \"\"\" Fully-connected neural network with one hidden layer.\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    n_output : int\n",
    "        Number of class labels.\n",
    "        \n",
    "    n_features : int\n",
    "        Number of input features.\n",
    "        \n",
    "    n_hidden : int\n",
    "        Number of hidden units.\n",
    "        \n",
    "    l2 : float\n",
    "        regularization parameter\n",
    "        0 means no regularization\n",
    "        \n",
    "    epochs : int\n",
    "        One Epoch is when the entire dataset is passed forward and backward through the neural network only once.\n",
    "        \n",
    "    lr : float\n",
    "        Learning rate.\n",
    "        \n",
    "    batchsize : int\n",
    "        Total number of training examples present in a single batch.\n",
    "        \n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    w1 : array, shape = [n_features, n_hidden_units]\n",
    "        Weight matrix for input layer -> hidden layer.\n",
    "    w2 : array, shape = [n_hidden_units, n_output_units]\n",
    "        Weight matrix for hidden layer -> output layer.\n",
    "    b1 : array, shape = [n_hidden_units, ]\n",
    "        Bias for input layer-> hidden layer.\n",
    "    b2 : array, shape = [n_output_units, ]\n",
    "        Bias for hidden layer -> output layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_output, n_features, n_hidden=30,\n",
    "                 l2=0.0, epochs=50, lr=0.001,\n",
    "                 batchsize=1):\n",
    "        self.n_output = n_output\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.batchsize = batchsize\n",
    "        self.cost_ = []\n",
    "        #TODO Initialize weights and biases with np.random.uniform or np.random.normal and specify the shape\n",
    "        # Note: self.weights1 contains W1 and b1; self.weights2 contains W2 and b2. see function initilize_weights() for more details.\n",
    "        self.weights1, self.weights2 = self.initialize_weights()\n",
    "        print(\"weights1.shape\", self.weights1.shape)\n",
    "        print(\"weights2.shape\", self.weights2.shape)\n",
    "        self.w1 = self.weights1[:,0:783]\n",
    "        self.b1 = self.weights1[:,784]\n",
    "        self.w2 = self.weights2[:,0:49]\n",
    "        self.b2 = self.weights1[:,50]\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Compute sigmoid function\"\"\"\n",
    "        #TODO Implement\n",
    "        #return (1+1/np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    def sigmoid_gradient(self, z):\n",
    "        \"\"\"Compute gradient of the sigmoid function\"\"\"\n",
    "        #TODO Implement\n",
    "        return (self.sigmoid(z) * (1 - self.sigmoid(z)))\n",
    "        \n",
    "    def softmax(self, z):\n",
    "        \"\"\"Compute softmax function.\n",
    "        Implement a stable version which \n",
    "        takes care of overflow and underflow.\n",
    "        \"\"\"  \n",
    "        m = np.amax(z,axis=1)\n",
    "        m = m[:,np.newaxis]\n",
    "        sums = np.sum(np.exp(z - m),axis=1)\n",
    "        sums = sums[:,np.newaxis]\n",
    "        return (np.exp(z - m) / sums)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, X, w1, w2):\n",
    "        \"\"\"Compute feedforward step\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        z2 : array,\n",
    "            Input of the hidden layer.\n",
    "        a2 : array,\n",
    "            Output of the hidden layer.\n",
    "        z3 : array,\n",
    "            Input of the output layer.\n",
    "        a3 : array,\n",
    "            Output of the output layer.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO Implement\n",
    "        a1 = self.add_bias_unit(X)\n",
    "        z2 = np.matmul(a1, w1.transpose())\n",
    "        a2 = self.sigmoid(z2)\n",
    "        a2_bias = self.add_bias_unit(a2)\n",
    "        z3 = np.matmul(a2_bias, w2.transpose())\n",
    "        a3 = self.softmax(z3)\n",
    "        return a1,z2,a2_bias,z3,a3\n",
    "\n",
    "    def L2_regularization(self, lambd, w1, w2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        #TODO Implement\n",
    "        return (lambd/2.0)*(np.sum(w1**2) + np.sum(w2**2))\n",
    "\n",
    "        \n",
    "    def loss(self, y_enc, output, w1, w2, epsilon=1e-12):\n",
    "        \"\"\"Implement the cross-entropy loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_enc : array, one-hot encoded labels.\n",
    "        \n",
    "        output : array, output of the output layer\n",
    "        \n",
    "        epsilon: used to turn log(0) into log(epsilon)\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        cost : float, total loss.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        regularizer_cost = 0\n",
    "        regularizer_cost += self.L2_regularization(self.l2, w1, w2)\n",
    "        pred_cost = - np.sum((y_enc * np.log(output+epsilon)) + ((1 - y_enc+epsilon) * np.log(1 - output)))\n",
    "        cost = pred_cost + regularizer_cost\n",
    "        return cost/y_enc.shape[0]\n",
    "\n",
    "    def compute_gradient(self, a1, a2, a3, z2, z3, y_enc, w1, w2):\n",
    "        \"\"\" Compute gradient using backpropagation.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        X  : array, Input.\n",
    "        a2 : array, output of the hidden layer.\n",
    "        a3 : array, output of the output layer.\n",
    "        z2 : array, input of the hidden layer.\n",
    "        y_enc : array, one-hot encoded labels.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        grad_1 : array, Gradient of the weight matrix w1.\n",
    "        grad_2 : array, Gradient of the weight matrix w2.\n",
    "        grad_3 : array, Gradient of the bias vector b1.\n",
    "        grad_4 : array, Gradient of the bias vector b2.\n",
    "        \"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        #del3 = self.sigmoid_gradient(z3) * (-y_enc+ a3)\n",
    "        #del3 = self.softmax_gradient(z3) * (a3 - y_enc)\n",
    "        del3 = (a3 - y_enc)\n",
    "        grad_2 = np.dot(del3.T, a2)\n",
    "        z2 = self.add_bias_unit(z2)\n",
    "        del2 = self.sigmoid_gradient(z2) * np.dot(del3, w2)\n",
    "        del2 = del2[:,1:]\n",
    "        grad_1 = np.dot(del2.T, a1)\n",
    "        \n",
    "        # Adding gradinet of regularizer terms\n",
    "        grad_2 += self.l2 * w2\n",
    "        grad_1 += self.l2 * w1\n",
    "        grad_3, grad_4 = grad_1[:,784], grad_2[0:50]\n",
    "        return grad_1,grad_2,grad_3, grad_4\n",
    "\n",
    "    def inference(self, X):\n",
    "        \"\"\"Predict class labels\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, Input layer with original features.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        y_pred : array, Predicted class labels.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO Implement\n",
    "        a1,z2,a2,z3,a3 = self.forward(X, self.weights1, self.weights2)\n",
    "        return np.argmax(a3, axis=1)\n",
    "    \n",
    "    def shuffle_train_data(self, X, Y):\n",
    "        \"\"\"called after each epoch\"\"\"\n",
    "        perm = np.random.permutation(Y.shape[0])\n",
    "        X_shuf = X[perm]\n",
    "        Y_shuf = Y[perm]\n",
    "        return X_shuf, Y_shuf\n",
    "        \n",
    "    def train(self, X, y, verbose=False):\n",
    "        \"\"\" Fit the model.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, Input.\n",
    "        y : array, Ground truth class labels.\n",
    "        verbose : bool, Print the training progress\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        #TODO Initialization\n",
    "        self.cost_ = []\n",
    "        \n",
    "        learning_rate = self.lr\n",
    "        num_batches = self.batchsize\n",
    "        batch_size = len(X_trainval) // num_batches\n",
    "        for epoch in range(self.epochs):\n",
    "            train_X, train_Y = self.shuffle_train_data(X_trainval,Y_trainval)            \n",
    "            for b in range(num_batches):\n",
    "                x_batch = train_X[b*batch_size:min(((b+1)*batch_size),len(X_trainval)),:]\n",
    "                y_batch = train_Y[b*batch_size:min(((b+1)*batch_size),len(X_trainval))]\n",
    "                y_enc = self.encode_labels(y_batch,self.n_output)\n",
    "                a1,z2,a2,z3,a3 = self.forward(x_batch,self.weights1,self.weights2)\n",
    "                # Calculate loss\n",
    "                cost = self.loss(y_enc, a3, self.weights1, self.weights2)\n",
    "                # Do backprop\n",
    "                grad1, grad2, grad_3, grad_4 = self.compute_gradient(a1, a2, a3, z2, z3, y_enc, self.weights1, self.weights2)\n",
    "                # Do one step of gradient descent using the gradients\n",
    "                self.weights1 -= learning_rate * grad1\n",
    "                self.weights2 -= learning_rate * grad2\n",
    "                self.w1 -= learning_rate * self.weights1[:,0:783]\n",
    "                self.b1 -= learning_rate * self.weights1[:,784]\n",
    "                self.w2 -= learning_rate * self.weights2[:,0:49]\n",
    "                self.b2 -= learning_rate * self.weights1[:,50]\n",
    "                # Store cost for the first plotting problem\n",
    "                costs_list_all.append(cost)\n",
    "                \n",
    "            # Update cost_ list after each epoch\n",
    "            y_enc_train = self.encode_labels(Y_trainval,self.n_output)\n",
    "            a1_train,z2_train,a2_train,z3_train,a3_train = self.forward(X_trainval,self.weights1,self.weights2)\n",
    "            cost_train = self.loss(y_enc_train, a3_train, self.weights1, self.weights2)\n",
    "            self.cost_.append(cost_train)\n",
    "            \n",
    "            if (verbose) and ((epoch % 10) == 0 or epoch == self.epochs - 1):\n",
    "                print (\"Epoch:\",epoch,\"Cost:\",self.cost_[-1])\n",
    "\n",
    "    # Additional functions defined for convenient implementation\n",
    "    def encode_labels(self, y, k):\n",
    "        arr = np.zeros((len(y),k))\n",
    "        for i in range(len(y)):\n",
    "            arr[i][y[i]] = 1\n",
    "        return arr\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        rand = np.random.RandomState(seed=1)\n",
    "        weights1 = rand.normal(size=(self.n_hidden,self.n_features+1)) * 0.0001\n",
    "        weights2 = rand.normal(size=(self.n_output,self.n_hidden+1)) * 0.0001\n",
    "        return weights1, weights2\n",
    "    \n",
    "    def softmax_gradient(self, z):\n",
    "        I = np.eye(z.shape[0],z.shape[1])\n",
    "        return self.softmax(z) * (I - self.softmax(z))\n",
    "\n",
    "    def add_bias_unit(self, X):\n",
    "        if len(X.shape) > 1:\n",
    "            le = X.shape[0]\n",
    "            b = np.ones((le,1))\n",
    "        else:\n",
    "            l = 1\n",
    "            b = np.ones(l)\n",
    "        X = np.concatenate((b,X),axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights1.shape (50, 785)\n",
      "weights2.shape (10, 51)\n"
     ]
    }
   ],
   "source": [
    "nn = Fully_connected_Neural_Network(n_output=10, \n",
    "                                    n_features=X_trainval.shape[1], \n",
    "                                    n_hidden=50, \n",
    "                                    l2=0.1, \n",
    "                                    epochs=1000, \n",
    "                                    lr=0.001,\n",
    "                                    batchsize=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Cost: 2.5366724129074716\n",
      "Epoch: 10 Cost: 0.8524639858034313\n",
      "Epoch: 20 Cost: 0.7162574689471092\n",
      "Epoch: 30 Cost: 0.6629876871936073\n",
      "Epoch: 40 Cost: 0.6290721187927946\n",
      "Epoch: 50 Cost: 0.6044998592911964\n",
      "Epoch: 60 Cost: 0.5842223041883026\n",
      "Epoch: 70 Cost: 0.5692228582299659\n",
      "Epoch: 80 Cost: 0.5555164552913749\n",
      "Epoch: 90 Cost: 0.5449187331024774\n",
      "Epoch: 100 Cost: 0.5339346010189218\n",
      "Epoch: 110 Cost: 0.5254499946318855\n",
      "Epoch: 120 Cost: 0.517040160705805\n",
      "Epoch: 130 Cost: 0.5105147454912969\n",
      "Epoch: 140 Cost: 0.5014044258433893\n",
      "Epoch: 150 Cost: 0.49562279181715674\n",
      "Epoch: 160 Cost: 0.48952832237159566\n",
      "Epoch: 170 Cost: 0.48694123905582054\n",
      "Epoch: 180 Cost: 0.4783409525963082\n",
      "Epoch: 190 Cost: 0.474636228876948\n",
      "Epoch: 200 Cost: 0.46980712969445243\n",
      "Epoch: 210 Cost: 0.4685452609455232\n",
      "Epoch: 220 Cost: 0.46426283587024025\n",
      "Epoch: 230 Cost: 0.4599284004855235\n",
      "Epoch: 240 Cost: 0.45524427743828016\n",
      "Epoch: 250 Cost: 0.4521984355432891\n",
      "Epoch: 260 Cost: 0.4482619503901943\n",
      "Epoch: 270 Cost: 0.44534916820495196\n",
      "Epoch: 280 Cost: 0.4425220283975041\n",
      "Epoch: 290 Cost: 0.43989028183950907\n",
      "Epoch: 300 Cost: 0.4381060960852518\n",
      "Epoch: 310 Cost: 0.4368792057610375\n",
      "Epoch: 320 Cost: 0.43265491478862017\n",
      "Epoch: 330 Cost: 0.4340011946911266\n",
      "Epoch: 340 Cost: 0.4289931755289243\n",
      "Epoch: 350 Cost: 0.4278198109785908\n",
      "Epoch: 360 Cost: 0.42854328050762847\n",
      "Epoch: 370 Cost: 0.42533680401723856\n",
      "Epoch: 380 Cost: 0.4213054768981939\n",
      "Epoch: 390 Cost: 0.4191714531537193\n",
      "Epoch: 400 Cost: 0.41811312671205453\n",
      "Epoch: 410 Cost: 0.41678026825856934\n",
      "Epoch: 420 Cost: 0.41537486027060927\n",
      "Epoch: 430 Cost: 0.41530685045706\n",
      "Epoch: 440 Cost: 0.4150433939326212\n",
      "Epoch: 450 Cost: 0.41148005672130455\n",
      "Epoch: 460 Cost: 0.4106696492862906\n",
      "Epoch: 470 Cost: 0.40990362953293885\n",
      "Epoch: 480 Cost: 0.406900036863227\n",
      "Epoch: 490 Cost: 0.40554970676898505\n",
      "Epoch: 500 Cost: 0.40542841601351637\n",
      "Epoch: 510 Cost: 0.40332184155512685\n",
      "Epoch: 520 Cost: 0.4031916806667924\n",
      "Epoch: 530 Cost: 0.4006919356341518\n",
      "Epoch: 540 Cost: 0.4000440720531866\n",
      "Epoch: 550 Cost: 0.40154922442758034\n",
      "Epoch: 560 Cost: 0.4005380442516874\n",
      "Epoch: 570 Cost: 0.40259796159523575\n",
      "Epoch: 580 Cost: 0.39803626092002126\n",
      "Epoch: 590 Cost: 0.3959050789963567\n",
      "Epoch: 600 Cost: 0.39631405976153233\n",
      "Epoch: 610 Cost: 0.3967133039027696\n",
      "Epoch: 620 Cost: 0.3954703794255037\n",
      "Epoch: 630 Cost: 0.3940605887831643\n",
      "Epoch: 640 Cost: 0.39549533643809526\n",
      "Epoch: 650 Cost: 0.39109316631161706\n",
      "Epoch: 660 Cost: 0.3924680083172697\n",
      "Epoch: 670 Cost: 0.39446233045433265\n",
      "Epoch: 680 Cost: 0.3902242294991032\n",
      "Epoch: 690 Cost: 0.3920148832027077\n",
      "Epoch: 700 Cost: 0.39019463723959563\n",
      "Epoch: 710 Cost: 0.3871930754621458\n",
      "Epoch: 720 Cost: 0.3965583342023453\n",
      "Epoch: 730 Cost: 0.3891183305096136\n",
      "Epoch: 740 Cost: 0.3855834574323419\n",
      "Epoch: 750 Cost: 0.3870530525666687\n",
      "Epoch: 760 Cost: 0.3840075806093145\n",
      "Epoch: 770 Cost: 0.38394219965269655\n",
      "Epoch: 780 Cost: 0.3838516736008388\n",
      "Epoch: 790 Cost: 0.38345028898644873\n",
      "Epoch: 800 Cost: 0.3926636178647145\n",
      "Epoch: 810 Cost: 0.38332194063265723\n",
      "Epoch: 820 Cost: 0.3810863075954372\n",
      "Epoch: 830 Cost: 0.38302746709342556\n",
      "Epoch: 840 Cost: 0.3857048654439347\n",
      "Epoch: 850 Cost: 0.381203999640543\n",
      "Epoch: 860 Cost: 0.3825619643694527\n",
      "Epoch: 870 Cost: 0.380776504483158\n",
      "Epoch: 880 Cost: 0.3803738717214324\n",
      "Epoch: 890 Cost: 0.3785120279861224\n",
      "Epoch: 900 Cost: 0.378037358394884\n",
      "Epoch: 910 Cost: 0.3824116612328914\n",
      "Epoch: 920 Cost: 0.37875931283153824\n",
      "Epoch: 930 Cost: 0.3824529482792925\n",
      "Epoch: 940 Cost: 0.376655227643241\n",
      "Epoch: 950 Cost: 0.37810171945460075\n",
      "Epoch: 960 Cost: 0.37850917767027537\n",
      "Epoch: 970 Cost: 0.3756560644671918\n",
      "Epoch: 980 Cost: 0.37755703354444875\n",
      "Epoch: 990 Cost: 0.3771983751319619\n",
      "Epoch: 999 Cost: 0.37636933085807023\n"
     ]
    }
   ],
   "source": [
    "nn.train(X_trainval, Y_trainval, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training error for every iteration\n",
    "# in every epoch\n",
    "\n",
    "# TODO Implement\n",
    "#print (len(nn.cost_))\n",
    "plt.plot(range(0,len(costs_list_all)),costs_list_all)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost vs Iterations')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training error in every epoch\n",
    "# TODO Implement\n",
    "print (len(nn.cost_))\n",
    "plt.plot(range(0,len(nn.cost_)), nn.cost_)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost vs Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 0 0 ... 3 0 5]\n",
      "[9 0 0 ... 3 0 5]\n",
      "Training accuracy: 94.65%\n",
      "[9 2 1 ... 8 1 5]\n",
      "[9 2 1 ... 8 1 5]\n",
      "Test accuracy: 88.39%\n"
     ]
    }
   ],
   "source": [
    "# Compute Training Accuracy\n",
    "# TODO Implement\n",
    "pred_labels_training = nn.inference(X_trainval)\n",
    "acc = 0\n",
    "print (pred_labels_training)\n",
    "print (Y_trainval)\n",
    "for index,val in np.ndenumerate(pred_labels_training):\n",
    "    if pred_labels_training[index] == Y_trainval[index]:\n",
    "        acc += 1\n",
    "acc /= Y_trainval.shape[0]\n",
    "print('Training accuracy: %.2f%%' % (acc * 100))\n",
    "\n",
    "# Compute Test Accuracy\n",
    "# TODO Implement\n",
    "pred_labels_test = nn.inference(X_test)\n",
    "acc = 0\n",
    "print (pred_labels_test)\n",
    "print (Y_test)\n",
    "for index,val in np.ndenumerate(pred_labels_test):\n",
    "    if pred_labels_test[index] == Y_test[index]:\n",
    "        acc += 1\n",
    "acc /= Y_test.shape[0]\n",
    "print('Test accuracy: %.2f%%' % (acc * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
